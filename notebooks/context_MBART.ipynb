{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we finetune MBART50 on machine translation and supply an additional context vector to the decoder to improve intertextuality \n",
    "\n",
    "# we will use the Hugging Face transformers library\n",
    "from transformers import MBartTokenizer, MBartForConditionalGeneration, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# set the seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# load the dataset\n",
    "data = pd.read_csv('data/eng_fra.csv')\n",
    "data = data.dropna()\n",
    "data = data.drop_duplicates()\n",
    "data = data.reset_index(drop=True)\n",
    "data.head()\n",
    "\n",
    "# split the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=seed)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# need to add a callback to the trainer to supply the context vector to the decoder\n",
    "class ContextCallback:\n",
    "    def __init__(self, tokenizer: MBartTokenizer, context: List[str]):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.context = context\n",
    "\n",
    "    def __call__(self, model, inputs):\n",
    "        inputs['decoder_input_ids'] = self.tokenizer(self.context, return_tensors='pt', padding=True).input_ids\n",
    "        return inputs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
